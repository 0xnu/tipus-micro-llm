{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-20T17:34:54.004195Z",
     "iopub.status.busy": "2025-08-20T17:34:54.003578Z",
     "iopub.status.idle": "2025-08-20T17:34:54.011540Z",
     "shell.execute_reply": "2025-08-20T17:34:54.011047Z",
     "shell.execute_reply.started": "2025-08-20T17:34:54.004171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from safetensors.torch import save_file\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from huggingface_hub import HfApi, Repository, upload_file, create_repo, upload_folder\n",
    "from huggingface_hub import login as hf_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T17:34:56.599056Z",
     "iopub.status.busy": "2025-08-20T17:34:56.598491Z",
     "iopub.status.idle": "2025-08-20T17:34:56.709707Z",
     "shell.execute_reply": "2025-08-20T17:34:56.709173Z",
     "shell.execute_reply.started": "2025-08-20T17:34:56.599030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# HuggingFace Authentication and Setup\n",
    "def setup_huggingface_auth():\n",
    "    \"\"\"Setup HuggingFace authentication\"\"\"\n",
    "    try:\n",
    "        # Option 1: Login with token (recommended for production)\n",
    "        # Get your token from https://huggingface.co/settings/tokens\n",
    "        hf_login(token=\"enter_your_token_here\")\n",
    "        \n",
    "        # Option 2: Interactive login\n",
    "        # hf_login()\n",
    "        \n",
    "        print(\"HuggingFace authentication successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFace authentication failed: {e}\")\n",
    "        print(\"Please ensure you have a valid HuggingFace token\")\n",
    "        return False\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "auth_success = setup_huggingface_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T17:38:18.890841Z",
     "iopub.status.busy": "2025-08-20T17:38:18.890558Z",
     "iopub.status.idle": "2025-08-20T17:38:21.506955Z",
     "shell.execute_reply": "2025-08-20T17:38:21.506203Z",
     "shell.execute_reply.started": "2025-08-20T17:38:18.890820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-3-270m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation='eager'  # Added eager attention\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded with eager attention: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T17:38:28.189886Z",
     "iopub.status.busy": "2025-08-20T17:38:28.189603Z",
     "iopub.status.idle": "2025-08-20T17:38:39.936893Z",
     "shell.execute_reply": "2025-08-20T17:38:39.936256Z",
     "shell.execute_reply.started": "2025-08-20T17:38:28.189865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    # Filter out empty texts\n",
    "    dataset = dataset.filter(lambda example: len(example[\"text\"].strip()) > 0)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Tokenise with proper padding and truncation\n",
    "        tokenised = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",  # Changed to max_length\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        # Labels are input_ids for causal LM\n",
    "        tokenised[\"labels\"] = tokenised[\"input_ids\"].copy()\n",
    "        return tokenised\n",
    "    \n",
    "    # Tokenise the dataset\n",
    "    tokenised_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenising dataset\"\n",
    "    )\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    tokenised_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    return tokenised_dataset\n",
    "\n",
    "train_dataset = prepare_dataset()\n",
    "print(f\"Training samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T17:38:44.680936Z",
     "iopub.status.busy": "2025-08-20T17:38:44.680389Z",
     "iopub.status.idle": "2025-08-20T17:38:44.708846Z",
     "shell.execute_reply": "2025-08-20T17:38:44.708104Z",
     "shell.execute_reply.started": "2025-08-20T17:38:44.680895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"./trained_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=False,  # Disabled FP16 to avoid gradient scaling issues\n",
    "    bf16=True,   # Use BF16 instead if available\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,  # Added gradient clipping\n",
    "    report_to=[],\n",
    "    dataloader_num_workers=0\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T17:38:47.512736Z",
     "iopub.status.busy": "2025-08-20T17:38:47.512450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fix the deprecation warning by using processing_class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,  # Changed from tokenizer to processing_class\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_save_dir = \"./gemma_3_270m_trained\"\n",
    "os.makedirs(final_save_dir, exist_ok=True)\n",
    "\n",
    "# Save model weights as safetensors\n",
    "state_dict = model.state_dict()\n",
    "safetensors_path = os.path.join(final_save_dir, \"model.safetensors\")\n",
    "save_file(state_dict, safetensors_path)\n",
    "\n",
    "# Save config.json\n",
    "config_dict = model.config.to_dict()\n",
    "with open(os.path.join(final_save_dir, \"config.json\"), 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Save generation_config.json\n",
    "generation_config = {\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "with open(os.path.join(final_save_dir, \"generation_config.json\"), 'w') as f:\n",
    "    json.dump(generation_config, f, indent=2)\n",
    "\n",
    "# Save tokenizer files (this creates all tokenizer-related files)\n",
    "tokenizer.save_pretrained(final_save_dir)\n",
    "\n",
    "print(f\"Model saved to {final_save_dir}\")\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(final_save_dir):\n",
    "    file_path = os.path.join(final_save_dir, file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  {file}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "required_files = [\n",
    "    \"added_tokens.json\",\n",
    "    \"config.json\", \n",
    "    \"generation_config.json\",\n",
    "    \"model.safetensors\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    if not os.path.exists(os.path.join(final_save_dir, file)):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Missing files: {missing_files}\")\n",
    "else:\n",
    "    print(\"âœ… All required files present\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
