{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc995cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb5509a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e51df70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyper-parameters\n",
    "# -----------------------------\n",
    "max_seq_length = 256  # Maximum sequence length for Q&A pairs\n",
    "batch_size = 32\n",
    "n_layer = 8  # Increased layers for better understanding\n",
    "n_head = 8\n",
    "n_embd = 768  # Larger embedding dimension\n",
    "dropout = 0.1\n",
    "max_iters = 10_000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 50\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f893ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Tokeniser (Word-level with special tokens)\n",
    "# -------------------------------------------\n",
    "class SimpleTokeniser:\n",
    "    def __init__(self, vocab_size_limit=10000):\n",
    "        self.vocab_size_limit = vocab_size_limit\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.sep_token = \"<SEP>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.special_tokens = [self.pad_token, self.unk_token, self.sep_token, self.eos_token]\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = self._tokenise_text(text)\n",
    "            for word in words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Sort by frequency and limit vocab size\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_words = [word for word, _ in sorted_words[:self.vocab_size_limit - len(self.special_tokens)]]\n",
    "        \n",
    "        # Build word-to-id mappings\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.word_to_id[token] = i\n",
    "            self.id_to_word[i] = token\n",
    "        \n",
    "        for i, word in enumerate(vocab_words, len(self.special_tokens)):\n",
    "            self.word_to_id[word] = i\n",
    "            self.id_to_word[i] = word\n",
    "        \n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "    \n",
    "    def _tokenise_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple word tokenisation\"\"\"\n",
    "        text = text.lower()\n",
    "        # Split on whitespace and punctuation\n",
    "        words = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
    "        return words\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        words = self._tokenise_text(text)\n",
    "        return [self.word_to_id.get(word, self.word_to_id[self.unk_token]) for word in words]\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        words = []\n",
    "        for id in token_ids:\n",
    "            if id in self.id_to_word:\n",
    "                word = self.id_to_word[id]\n",
    "                if word not in self.special_tokens:\n",
    "                    words.append(word)\n",
    "        return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a372597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Dataset for Q&A pairs\n",
    "# ----------------------\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs: List[Tuple[str, str]], tokeniser: SimpleTokeniser, max_length: int):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokeniser = tokeniser\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.qa_pairs[idx]\n",
    "        \n",
    "        # Encode question and answer with separator\n",
    "        q_tokens = self.tokeniser.encode(question)\n",
    "        a_tokens = self.tokeniser.encode(answer)\n",
    "        \n",
    "        # Create input: [Question] <SEP> [Answer] <EOS>\n",
    "        sep_id = self.tokeniser.word_to_id[self.tokeniser.sep_token]\n",
    "        eos_id = self.tokeniser.word_to_id[self.tokeniser.eos_token]\n",
    "        pad_id = self.tokeniser.word_to_id[self.tokeniser.pad_token]\n",
    "        \n",
    "        input_ids = q_tokens + [sep_id] + a_tokens + [eos_id]\n",
    "        \n",
    "        # Truncate if too long (leave room for shifting)\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        \n",
    "        # Pad if too short\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + [pad_id] * padding_length\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1 if id != pad_id else 0 for id in input_ids]\n",
    "        \n",
    "        # For training: input is all tokens except last, targets are all tokens except first\n",
    "        input_tensor = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
    "        target_tensor = torch.tensor(input_ids[1:], dtype=torch.long)\n",
    "        attention_tensor = torch.tensor(attention_mask[:-1], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_tensor,\n",
    "            'targets': target_tensor,\n",
    "            'attention_mask': attention_tensor\n",
    "        }\n",
    "\n",
    "# Load/Create Q&A data\n",
    "def load_qa_data() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Load or create Q&A pairs for training\"\"\"\n",
    "    qa_file = Path(\"./data/qa_pairs.json\")\n",
    "    \n",
    "    if qa_file.exists():\n",
    "        # Load existing Q&A pairs\n",
    "        with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "            qa_pairs = json.load(f)\n",
    "    else:\n",
    "        # Create sample Q&A pairs if file doesn't exist\n",
    "        qa_pairs = [\n",
    "            (\"What is the capital of France?\", \"The capital of France is Paris.\"),\n",
    "            (\"Who wrote Romeo and Juliet?\", \"William Shakespeare wrote Romeo and Juliet.\"),\n",
    "            (\"What is the largest planet in our solar system?\", \"Jupiter is the largest planet in our solar system.\"),\n",
    "            (\"When was the Declaration of Independence signed?\", \"The Declaration of Independence was signed in 1776.\"),\n",
    "            (\"What is photosynthesis?\", \"Photosynthesis is the process by which plants convert light energy into chemical energy.\"),\n",
    "            (\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"),\n",
    "            (\"What is the speed of light?\", \"The speed of light is approximately 299,792,458 metres per second.\"),\n",
    "            (\"What is machine learning?\", \"Machine learning is a type of artificial intelligence that enables computers to learn from data.\"),\n",
    "            (\"What is the smallest unit of matter?\", \"The atom is the smallest unit of matter that retains the properties of an element.\"),\n",
    "            (\"Who discovered gravity?\", \"Sir Isaac Newton discovered the law of universal gravitation.\")\n",
    "        ]\n",
    "        \n",
    "        # Save for future use\n",
    "        qa_file.parent.mkdir(exist_ok=True)\n",
    "        with open(qa_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d9a37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Model Architecture\n",
    "# -------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear transformations and split into heads\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        # Causal mask for autoregressive generation\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).bool()\n",
    "        scores.masked_fill_(~causal_mask, -1e9)\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention(self.norm1(x), mask)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_output\n",
    "        \n",
    "        return x\n",
    "\n",
    "class QATransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = n_embd\n",
    "        self.embedding = nn.Embedding(vocab_size, self.d_model)\n",
    "        self.pos_encoding = PositionalEncoding(self.d_model, max_seq_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(self.d_model, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(self.d_model)\n",
    "        self.fc_out = nn.Linear(self.d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, targets=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attention_mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.fc_out(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Ensure logits and targets have compatible shapes\n",
    "            assert logits.shape[:2] == targets.shape, f\"Shape mismatch: logits {logits.shape[:2]} vs targets {targets.shape}\"\n",
    "            \n",
    "            # Calculate loss only on non-padded tokens\n",
    "            pad_id = 0  # Padding token ID\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "            loss = loss_fct(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, tokeniser, max_new_tokens=50, temperature=1.0):\n",
    "        \"\"\"Generate answer given a question\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions for last token\n",
    "            logits, _ = self(input_ids)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token.item() == tokeniser.word_to_id[tokeniser.eos_token]:\n",
    "                break\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455f7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Training\n",
    "# ---------\n",
    "def train_model():\n",
    "    # Load data\n",
    "    qa_pairs = load_qa_data()\n",
    "    \n",
    "    # Split into train and validation\n",
    "    n_train = int(0.9 * len(qa_pairs))\n",
    "    train_pairs = qa_pairs[:n_train]\n",
    "    val_pairs = qa_pairs[n_train:]\n",
    "    \n",
    "    # Build tokeniser\n",
    "    tokeniser = SimpleTokeniser()\n",
    "    all_texts = [q + \" \" + a for q, a in qa_pairs]\n",
    "    tokeniser.build_vocab(all_texts)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = QADataset(train_pairs, tokeniser, max_seq_length)\n",
    "    val_dataset = QADataset(val_pairs, tokeniser, max_seq_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialise model\n",
    "    model = QATransformer(tokeniser.vocab_size).to(device)\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=max_iters)\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(max_iters // len(train_loader) + 1):\n",
    "        for batch in train_loader:\n",
    "            if global_step >= max_iters:\n",
    "                break\n",
    "                \n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, loss = model(input_ids, attention_mask, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimiser.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Evaluation\n",
    "            if global_step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                val_losses = []\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in val_loader:\n",
    "                        val_input = val_batch['input_ids'].to(device)\n",
    "                        val_targets = val_batch['targets'].to(device)\n",
    "                        val_mask = val_batch['attention_mask'].to(device)\n",
    "                        _, val_loss = model(val_input, val_mask, val_targets)\n",
    "                        val_losses.append(val_loss.item())\n",
    "                \n",
    "                avg_val_loss = sum(val_losses) / len(val_losses) if val_losses else float('inf')\n",
    "                print(f\"Step {global_step}: train loss = {loss.item():.4f}, val loss = {avg_val_loss:.4f}\")\n",
    "                model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "    \n",
    "    # Save model and tokeniser\n",
    "    timestamp = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = Path(\"models/\")\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model_file = save_dir / f\"model_{timestamp}.pt\"\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    \n",
    "    # Save tokeniser\n",
    "    tokeniser_data = {\n",
    "        \"word_to_id\": tokeniser.word_to_id,\n",
    "        \"id_to_word\": {str(k): v for k, v in tokeniser.id_to_word.items()},\n",
    "        \"vocab_size\": tokeniser.vocab_size,\n",
    "        \"special_tokens\": tokeniser.special_tokens\n",
    "    }\n",
    "    with open(save_dir / f\"tokeniser_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(tokeniser_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"n_layer\": n_layer,\n",
    "        \"n_head\": n_head,\n",
    "        \"n_embd\": n_embd,\n",
    "        \"dropout\": dropout,\n",
    "        \"vocab_size\": tokeniser.vocab_size\n",
    "    }\n",
    "    with open(save_dir / f\"config_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved to {model_file}\")\n",
    "    return model, tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05c01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Inference\n",
    "# ----------\n",
    "class QAInference:\n",
    "    def __init__(self, model_dir: str = \"models/\", device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        model_path = Path(model_dir)\n",
    "        \n",
    "        # Find latest files\n",
    "        model_files = sorted(model_path.glob(\"model_*.pt\"))\n",
    "        if not model_files:\n",
    "            raise ValueError(f\"No model files found in {model_dir}\")\n",
    "        \n",
    "        latest_model = model_files[-1]\n",
    "        timestamp = latest_model.stem.split('_')[1]\n",
    "        \n",
    "        # Load config\n",
    "        with open(model_path / f\"config_{timestamp}.json\", 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Load tokeniser\n",
    "        with open(model_path / f\"tokeniser_{timestamp}.json\", 'r') as f:\n",
    "            tokeniser_data = json.load(f)\n",
    "        \n",
    "        self.tokeniser = SimpleTokeniser()\n",
    "        self.tokeniser.word_to_id = tokeniser_data[\"word_to_id\"]\n",
    "        self.tokeniser.id_to_word = {int(k): v for k, v in tokeniser_data[\"id_to_word\"].items()}\n",
    "        self.tokeniser.vocab_size = tokeniser_data[\"vocab_size\"]\n",
    "        self.tokeniser.special_tokens = tokeniser_data[\"special_tokens\"]\n",
    "        \n",
    "        # Load model\n",
    "        self.model = QATransformer(config[\"vocab_size\"]).to(device)\n",
    "        self.model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def answer_question(self, question: str, max_length: int = 50, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate answer for a given question\"\"\"\n",
    "        # Prepare input\n",
    "        input_text = question + \" \" + self.tokeniser.sep_token\n",
    "        input_ids = self.tokeniser.encode(input_text)\n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Generate answer\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(input_tensor, self.tokeniser, max_length, temperature)\n",
    "        \n",
    "        # Decode output\n",
    "        output_tokens = output_ids[0].tolist()\n",
    "        \n",
    "        # Find separator token position\n",
    "        sep_id = self.tokeniser.word_to_id[self.tokeniser.sep_token]\n",
    "        if sep_id in output_tokens:\n",
    "            sep_idx = output_tokens.index(sep_id)\n",
    "            answer_tokens = output_tokens[sep_idx + 1:]\n",
    "        else:\n",
    "            answer_tokens = output_tokens[len(input_ids):]\n",
    "        \n",
    "        # Remove special tokens and decode\n",
    "        answer = self.tokeniser.decode(answer_tokens)\n",
    "        return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Main Function\n",
    "# -------------\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokeniser = train_model()\n",
    "    inference = QAInference()\n",
    "    question = \"What is the capital of France?\"\n",
    "    answer = inference.answer_question(question)\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
